{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcV4Tdx/la9oPwZIZwPtIs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarnixVos/Computer-Science/blob/main/CS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_7RIaAoa5kE",
        "outputId": "125e8487-af4d-4830-a192-83b197e8b842"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K6iBsRobUWL"
      },
      "source": [
        "#Import packages\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import random\n",
        "\n",
        "from random import shuffle\n",
        "from pprint import pprint\n",
        "from itertools import combinations\n",
        "from sympy import isprime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK3HKHR0a7dr",
        "outputId": "800be007-9a05-463b-c89c-781acebe4c22"
      },
      "source": [
        "# grab JSON file\n",
        "use_real = True\n",
        "file_real = \"/content/gdrive/MyDrive/CS/TVs-all-merged.json\"\n",
        "file_test = \"/content/gdrive/MyDrive/CS/TVs-minimal.json\"\n",
        "# file_test = \"test.json\"\n",
        "\n",
        "if(use_real):\n",
        "    with open(file_real) as json_file:\n",
        "        input = json.load(json_file)\n",
        "else:\n",
        "    with open(file_test) as json_file:\n",
        "        input = json.load(json_file)\n",
        "\n",
        "print(\"Data loaded!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "#k = 5  # shingle length\n",
        "m = 20  # minhash dense vector length\n",
        "b = 4  # amount of bands for LSH (make sure m is a multiple of b)\n",
        "bootstraps = 10\n",
        "sim_threshold = 0.3\n",
        "obs = 1000\n",
        "print(\"Hyper Params set\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtMl4ufvnNW0",
        "outputId": "0d91ebcd-ff77-4b64-fd3c-07be225ef330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyper Params set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttiBZRR8aoah"
      },
      "source": [
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "#_----------------------------------------------------HELPER FUNCTIONS--------------------------------------------#\n",
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "# Extract data\n",
        "def create_data_dict(data):\n",
        "  dct = {}\n",
        "  mapping_dict = {}\n",
        "  counter = 0 \n",
        "  for key in data:\n",
        "      products = data[key]\n",
        "      for i in range(0, len(products)):\n",
        "          title = data[key][i].get('title')\n",
        "          mapping_dict.update({counter : key + \"|||\" + data[key][i].get('shop')})\n",
        "          dct.update({key + \"|||\" + data[key][i].get('shop'): title})\n",
        "          counter +=1\n",
        "  return dct, mapping_dict\n",
        "\n",
        "def create_model_words(title):\n",
        "\n",
        "  #Standardize inch values\n",
        "  inch_values = [\"Inch\", 'inches', '\\\"', '-inch', ' inch']\n",
        "  for inch_value in inch_values:\n",
        "    title = title.replace(inch_value, 'inch')\n",
        "\n",
        "  #Standardize Hz values\n",
        "  hz_values = [\"Hertz\", 'hertz', 'Hz', 'HZ', '-hz']\n",
        "  for hz_value in hz_values:\n",
        "    title = title.replace(hz_value, 'hz')\n",
        "\n",
        "  #Remove site titles\n",
        "  sites = ['TheNerds.net', 'Newegg.com']\n",
        "  for site in sites:\n",
        "    title = title.replace(site, '')\n",
        "\n",
        "  #All to lower case\n",
        "  title = title.lower()\n",
        "  \n",
        "\n",
        "  model_words = re.findall(\"[a-zA-Z0-9\\.]+\", title)\n",
        "  words_1 = [string for string in model_words if not string.isalpha()]\n",
        "  words = [string for string in words_1 if not string.isnumeric()]\n",
        "\n",
        "  return words\n",
        "\n",
        "# Shingling function\n",
        "def create_shingles(doc, k):\n",
        "    shingles = set()\n",
        "    length = len(doc)\n",
        "    for index in range(length-k+1):\n",
        "        shingle = doc[index:index+k]\n",
        "        shingles.add(shingle)\n",
        "\n",
        "    return shingles\n",
        "\n",
        "\n",
        "# one hot encode shingle\n",
        "def encode_one_hot(shingles, vocab):\n",
        "    sparse_vec =  np.zeros(len(vocab))\n",
        "    for shingle in shingles:\n",
        "        idx = vocab.get(shingle)\n",
        "        sparse_vec[idx] = 1\n",
        "        # vocab_size = size of the vocab\n",
        "    return sparse_vec\n",
        "\n",
        "def hash_function(a,b,p,x):\n",
        "  return (a + b*x)\n",
        "\n",
        "\n",
        "# We have to save our hash functions and reuse them, so we have 20 different hash functions. Not 20x how ever many sparse vectors we have\n",
        "# def create_hash_funcs(vocab_size, m):\n",
        "#     hashes = np.zeros((m, vocab_size))\n",
        "#     for i in range(m):\n",
        "#         hash = np.random.permutation(len(vocab)) + 1\n",
        "#         hashes[i, :] = hash.copy()    \n",
        "#     return hashes.astype(int)\n",
        "\n",
        "\n",
        "#We have to save our hash functions and reuse them, so we have 20 different hash functions. Not 20x how ever many sparse vectors we have\n",
        "def create_hash_funcs(vocab_size, m):\n",
        "    hashes = np.zeros((m, vocab_size))\n",
        "    primes = [i for i in range(m,m*m) if isprime(i)]\n",
        "    for i in range(m):\n",
        "        a = random.randint(1, 100)\n",
        "        b = random.randint(1, 100)\n",
        "        p = np.repeat(random.choice(primes), vocab_size)\n",
        "        f = lambda x: a+b*x\n",
        "        base = np.arange(vocab_size)+1\n",
        "        func = f(base)\n",
        "        hash = np.mod(func, p)\n",
        "        hashes[i, :] = hash.copy()    \n",
        "    return hashes.astype(int)\n",
        "\n",
        "# create signatures\n",
        "# hash = a hash from create_hash_func\n",
        "# sparse_vec = the original sparse vector\n",
        "# vocab_size = size of the vocabulary\n",
        "\n",
        "\n",
        "# create complete signature vector\n",
        "\n",
        "def create_minhash_signature_vector(sparse_vec, vocab_size, m, hashes):\n",
        "    idxs_to_check = np.nonzero(sparse_vec)[0].tolist()\n",
        "    hash_values = hashes[:, idxs_to_check]\n",
        "    signatures = np.min(hash_values, axis=1)\n",
        "    return signatures\n",
        "\n",
        "# calculate Jacard Similarity\n",
        "\n",
        "\n",
        "def calculate_jacard_similarity(a, b):\n",
        "    return len(a.intersection(b)) / len(a.union(b))\n",
        "\n",
        "\n",
        "# split vector into bands\n",
        "def create_bands(signature, b):\n",
        "    r = int(len(signature)/b)\n",
        "    bands = []\n",
        "    for i in range(0, len(signature), r):\n",
        "        val = str(signature[i: i+r])\n",
        "        bands.append(val)\n",
        "    return np.stack(bands)\n",
        "\n",
        "\n",
        "#Get canidate pairs\n",
        "def get_canidate_pairs(buckets): \n",
        "  canidates= []\n",
        "  for key in buckets: \n",
        "    products_in_bucket = buckets[key]\n",
        "    if len(products_in_bucket) > 1:\n",
        "      canidates.extend(combinations(products_in_bucket,2))\n",
        "  return set(canidates)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_canidate_pairs(canidates):\n",
        "  canidate_pairs = []\n",
        "  for canidate_bucket in canidates:\n",
        "    canidate_list = canidate_bucket.strip('][').split(', ')\n",
        "    n = len(canidate_list)\n",
        "    for i in range(n):\n",
        "      for j in range(i+1, n):\n",
        "        canidate_pairs.append([canidate_list[i],canidate_list[j]])\n",
        "\n",
        "  return canidate_pairs\n",
        "\n",
        "# find match between bands, only need 1 band to match to hash into the same bucket\n",
        "\n",
        "\n",
        "def detect_equal_signature(sig1, sig2):\n",
        "    bands1 = split_signature(sig1, b)\n",
        "    bands2 = split_signature(sig2, b)\n",
        "    for band_1, band_2 in zip(bands1, bands2):\n",
        "        if band_1 == band_2:\n",
        "            print(\"MATCH FOUND\")\n",
        "            break\n",
        "\n",
        "def evaluate_pairs(pairs, mapping_dict,data): \n",
        "  #Evaluate pairs that we have marked as duplicates\n",
        "  TP = []\n",
        "  FP = []\n",
        "\n",
        "  for pair in pairs: \n",
        "    i1 = int(pair[0])\n",
        "    i2 = int(pair[1])\n",
        "    if(i1 == i2):\n",
        "      pass\n",
        "    else:\n",
        "      p1 = str.split(mapping_dict.get(i1),'|||')\n",
        "      p2 = str.split(mapping_dict.get(i2),'|||')\n",
        "\n",
        "      data1 = data.get(p1[0])\n",
        "      data2 = data.get(p2[0])\n",
        "\n",
        "      model1 = data.get(p1[0])[0]['modelID']\n",
        "      model2 = data.get(p2[0])[0]['modelID']\n",
        "\n",
        "      if (model1 == model2):\n",
        "        TP.append(pair)\n",
        "      else: \n",
        "        FP.append(pair)\n",
        "\n",
        "  return [len(TP) , len(FP)]\n",
        "\n",
        "def equal_tv_brands(title1, title2):\n",
        "    #If the brands are recognized and not equal, we mark as not equal \n",
        "    tv_brands = ['philips', 'supersonic', 'sharp', 'samsung', \"nec\", 'tcl', 'toshiba', 'hisense', 'sony', 'lg', 'sanyo', \n",
        "                 'coby', 'panasonic', 'rca', 'sunbritetv', 'jvc', 'insignia', 'haier', 'optoma', 'vizio', 'westinghouse',\n",
        "                 'sansui']\n",
        "    brand1 = \".\"\n",
        "    brand2 = \".\"\n",
        "\n",
        "    #print(f\"TV1: {str.lower(product_data1['title'])} TV2: {str.lower(product_data2['title'])}\")\n",
        "    for tv_brand in tv_brands: \n",
        "      \n",
        "      if tv_brand in title1:\n",
        "        brand1 = tv_brand\n",
        "      if tv_brand in title2:\n",
        "        brand2 = tv_brand\n",
        "    \n",
        "    if(brand1 is not brand2):\n",
        "      if (brand1 is \".\" or brand2 is \".\"):\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "    else: \n",
        "      return True\n",
        "\n",
        "def equal_resolutions(title1, title2):\n",
        "  resolution1 = \".\"\n",
        "  resolution2 = \".\"\n",
        "  resolutions = ['720p', '1080p']\n",
        "  for resolution in resolutions: \n",
        "    if resolution in title1:\n",
        "      resultion1 = resolution\n",
        "    if resolution in title2:\n",
        "      resultion2 = resolution\n",
        "\n",
        "  if(resolution1 is not resolution2):\n",
        "      if (resolution1 is \".\" or resolution2 is \".\"):\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "  else: \n",
        "      return True\n",
        "\n",
        "def equal_refresh_rates(title1, title2):\n",
        "    refresh_rate1 = '.'\n",
        "    refresh_rate2 = \".\"\n",
        "    refresh_rates = ['60hz', '120hz', '600hz', '240hz']\n",
        "    for refresh_rate in refresh_rates: \n",
        "      if refresh_rate in title1: \n",
        "        refresh_rate1 = refresh_rate\n",
        "      if refresh_rate in title2: \n",
        "        refresh_rate2 = refresh_rate\n",
        "    \n",
        "    if(refresh_rate1 is not refresh_rate2):\n",
        "      if (refresh_rate1 is \".\" or refresh_rate2 is \".\"):\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "    else: \n",
        "      return True\n",
        "\n",
        "\n",
        "def standardize_product_title(title): \n",
        "    title = str.lower(title)\n",
        "    #Standardize inch values\n",
        "    inch_values = [\"inch\", 'inches', '\\\"', '-inch', ' inch']\n",
        "    for inch_value in inch_values:\n",
        "      title = title.replace(inch_value, 'inch')\n",
        "\n",
        "    #Standardize Hz values\n",
        "    hz_values = ['hertz', '-hz', ' hz']\n",
        "    for hz_value in hz_values:\n",
        "      title = title.replace(hz_value, 'hz')\n",
        "    \n",
        "    return title\n",
        "\n",
        "\n",
        "\n",
        "def get_duplicates(canidates, mapping_dict, data, matrix):\n",
        "  #Now we analyze the canidate pairs \n",
        "  duplicates = []\n",
        "  for pair in canidates: \n",
        "    i1 = int(pair[0])\n",
        "    i2 = int(pair[1])\n",
        "  \n",
        "    #check if its the exact same product, in that case dont mark as duplicate pair\n",
        "    if(i1 == i2):\n",
        "      pass\n",
        "    else:  \n",
        "      #Grab the product key and correspoding store\n",
        "      p1 = str.split(mapping_dict.get(i1),'|||')\n",
        "      p2 = str.split(mapping_dict.get(i2),'|||')\n",
        "\n",
        "      #Get original data set for product 1 \n",
        "      products1 = data.get(p1[0], )\n",
        "      for product in products1: \n",
        "        if product['shop'] == p1[1]:\n",
        "          product_data1 = product\n",
        "\n",
        "      #Get original data set for product 12\n",
        "      products2 = data.get(p2[0], )\n",
        "      for product in products2: \n",
        "        if product['shop'] == p2[1]:\n",
        "          product_data2 = product\n",
        "\n",
        "      #Now we need to classify if, based on these two product sets, the models are indeed equal\n",
        "      title1 = standardize_product_title(product_data1['title'])\n",
        "      title2 = standardize_product_title(product_data2['title'])\n",
        "      \n",
        "      if(equal_tv_brands(title1, title2)):\n",
        "        #Are the resolutions equal:\n",
        "        if(equal_resolutions(title1, title2)):\n",
        "          #Are the refresh rates equal\n",
        "          if(equal_refresh_rates(title1, title2)):\n",
        "            #Is the sim at least above the threshold: \n",
        "            idxs1 = np.nonzero(matrix[i1, :])[0].tolist()\n",
        "            idxs2 = np.nonzero(matrix[i2, :])[0].tolist()\n",
        "            sim = calculate_jacard_similarity(set(idxs1), set(idxs2))\n",
        "            if (sim) > sim_threshold:\n",
        "              duplicates.append(pair)\n",
        "            \n",
        "\n",
        "\n",
        "  return duplicates\n",
        "\n",
        "#Get total amount of duplicate pairs in the data\n",
        "def get_total_duplicate_pairs(data):\n",
        "  total = 0 \n",
        "  for key in data: \n",
        "    l = len(data[key])\n",
        "    if l > 1: \n",
        "      for i in range(1,l):\n",
        "        total += i\n",
        "  return total\n",
        "\n",
        "def get_total_amount_products(data): \n",
        "  total = 0\n",
        "  max_comps =0 \n",
        "  for key in data: \n",
        "    l = len(data[key])\n",
        "    total += l \n",
        "  for i in range(1, total): \n",
        "    max_comps += i \n",
        "\n",
        "\n",
        "  return total, max_comps\n",
        "#We now have canidate pairs, evaluate performance\n",
        "def evaluate_performance(canidates, duplicates, data, mapping_dict):\n",
        "  total_comps = len(canidates)\n",
        "  total_dups = get_total_duplicate_pairs(data)\n",
        "  TP, FN = evaluate_pairs(duplicates, mapping_dict, data)\n",
        "  recall = TP / total_dups\n",
        "  precision = TP / (TP + FN)\n",
        "  PQ = TP / total_comps\n",
        "  PC = TP / total_dups\n",
        "  total_products, max_comps = get_total_amount_products(data)\n",
        "  frac_of_comp = total_comps / max_comps\n",
        "  F1 = 2*precision*recall / (precision + recall)\n",
        "  F1_star =  2*PQ*PC / (PQ + PC)\n",
        "\n",
        "  performance = {\n",
        "    \"F1\": F1,\n",
        "    \"F1_star\": F1_star,\n",
        "    \"PQ\": PQ,\n",
        "    \"PC\": PC,\n",
        "    \"Recall\": recall,\n",
        "    \"Precision\": precision,\n",
        "    \"Frac Comps\": frac_of_comp,\n",
        "    \"Total products\": total_products\n",
        "  }\n",
        "\n",
        "  return performance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4MbIj3GbohC"
      },
      "source": [
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "#_-------------------------------------------------------MAIN FUNCTION--------------------------------------------#\n",
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "\n",
        "# This part extracts all the titles and adds them to a dict\n",
        "# ISSUE: duplicate items get filed under the same key, but we want different keys here for comparisons >  SOLUTION use key + shop as unique identifier\n",
        "\n",
        "\n",
        "def main_func(data):\n",
        "  title_dict, mapping_dict = create_data_dict(data)\n",
        "  #print(\"------ EXTRACTED ALL THE TITLES -------\")\n",
        "\n",
        "  # Now we create a list of all our model words and also create our vocab for one hot encoding\n",
        "  model_words_dict = {}\n",
        "  vocab = set()\n",
        "  for key in title_dict:\n",
        "      title = title_dict[key]\n",
        "      model_words = create_model_words(title)\n",
        "      model_words_dict.update({key: model_words})\n",
        "      vocab = set.union(vocab, model_words)\n",
        "  #print(\"------ CREATED MODELWORDS -------\")\n",
        "  # print(\"VOCAB: \", vocab)\n",
        "  #pprint(model_words_dict)\n",
        "\n",
        "\n",
        "  total_model_words = len(vocab)\n",
        "\n",
        "  # CREATE THE MINHASH MATRIX\n",
        "  # we need a dictionary for the vocab to get O(1) access time\n",
        "  vocab_dict = {}\n",
        "\n",
        "  i = 0\n",
        "  for model_word in vocab:\n",
        "      vocab_dict.update({model_word: i})\n",
        "      i = i+1\n",
        "\n",
        "  #Create one hot vector representation\n",
        "  matrix = []\n",
        "  for key in model_words_dict:\n",
        "      model_words_set = model_words_dict[key]\n",
        "      matrix.append(encode_one_hot(model_words_set, vocab_dict))\n",
        "  #print(\"------ CREATED MATRIX SPARSE VEC REPRESENTATION -------\")\n",
        "  matrix = np.stack(matrix)\n",
        "  #print(matrix.shape)\n",
        "\n",
        "\n",
        "  # Create MinHash Matrix with signature of length m\n",
        "  signature_matrix = []\n",
        "  hashes = create_hash_funcs(total_model_words, m)\n",
        "  for vector in matrix:\n",
        "    signature_matrix.append(create_minhash_signature_vector(vector, total_model_words, m, hashes))\n",
        "\n",
        "  #print(\"------ CREATED MINHASH DATAFRAME -------\")\n",
        "  signature_matrix = np.stack(signature_matrix)\n",
        "  #print(signature_matrix, signature_matrix.shape)\n",
        "\n",
        "\n",
        "  #Generate dictionary of buckets\n",
        "  buckets = {}\n",
        "\n",
        "  i = 0\n",
        "  for signature in signature_matrix:\n",
        "    bands = create_bands(signature, b)\n",
        "    for band in bands: \n",
        "      if band in buckets: \n",
        "        buckets[band].append(i)\n",
        "      else:\n",
        "        buckets.update({band : [i]})\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  #print(\"------ CREATED LSH BUCKETS -------\")\n",
        "\n",
        "\n",
        "  #get canidate pairs\n",
        "  canidates = get_canidate_pairs(buckets)\n",
        "  #print(\"------ CREATED CANIDATE PAIRS -------\")\n",
        "\n",
        "\n",
        "  duplicates = get_duplicates(canidates, mapping_dict, data, matrix)\n",
        "  #print(\"------ DETERMINED DUPLICATE PAIRS -------\")\n",
        "\n",
        "  performance = evaluate_performance(canidates, duplicates, data, mapping_dict)\n",
        "  return performance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "#_-------------------------------------------------------BOOTSTRAPPING--------------------------------------------#\n",
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "performances = []\n",
        "_ , main_mapping_dict = create_data_dict(input)\n",
        "for z in range(1,bootstraps+1):\n",
        "  print(\"Bootstrap \", z)\n",
        "  bootstrap_data = {}\n",
        "  for _ in range(obs):\n",
        "    idx = random.randint(0,1623)\n",
        "    product_name, product_store = str.split(main_mapping_dict.get(idx),'|||')\n",
        "    products = input.get(product_name)\n",
        "    for product in products: \n",
        "      if product['shop'] == product_store:\n",
        "        product_dict = product\n",
        "        product_key = product_dict['modelID']\n",
        "\n",
        "    if product_key in bootstrap_data:\n",
        "      bootstrap_data[product_key].append(product_dict)\n",
        "    else: \n",
        "      bootstrap_data.update({product_key : [product_dict]})\n",
        "  performance = main_func(bootstrap_data)\n",
        "  performances.append(performance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQUAsN7Kx5ar",
        "outputId": "b9e8065c-263e-47ff-b490-d07883f55d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap  1\n",
            "Bootstrap  2\n",
            "Bootstrap  3\n",
            "Bootstrap  4\n",
            "Bootstrap  5\n",
            "Bootstrap  6\n",
            "Bootstrap  7\n",
            "Bootstrap  8\n",
            "Bootstrap  9\n",
            "Bootstrap  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(performances)\n",
        "means = df.mean(axis=0)\n",
        "print(means)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izGMs2L87Hef",
        "outputId": "a747e502-349f-4dce-ffc9-e31d1943ec6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1                   0.034952\n",
            "F1_star              0.010671\n",
            "PQ                   0.006581\n",
            "PC                   0.028852\n",
            "Recall               0.028852\n",
            "Precision            0.044999\n",
            "Frac Comps           0.004129\n",
            "Total products    1000.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aTsfODRk06rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance = evaluate_performance(canidates, duplicates, data)\n",
        "pprint(performance)"
      ],
      "metadata": {
        "id": "rlrx5dTAgty_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we analyze the canidate pairs \n",
        "canidates2 = []\n",
        "for pair in canidates: \n",
        "  i1 = int(pair[0])\n",
        "  i2 = int(pair[1])\n",
        "  \n",
        "\n",
        "\n",
        "  #check if its the exact same product, in that case dont mark as duplicate pair\n",
        "  if(i1 == i2):\n",
        "    pass\n",
        "  else: \n",
        "    \n",
        "    \n",
        "    #Grab the product key and correspoding store\n",
        "    p1 = str.split(mapping_dict.get(i1),'|||')\n",
        "    p2 = str.split(mapping_dict.get(i2),'|||')\n",
        "\n",
        "    #Get original data set for product 1 \n",
        "    products1 = data.get(p1[0], )\n",
        "    for product in products1: \n",
        "      if product['shop'] == p1[1]:\n",
        "        product_data1 = product\n",
        "\n",
        "    #Get original data set for product 12\n",
        "    products2 = data.get(p2[0], )\n",
        "    for product in products2: \n",
        "      if product['shop'] == p2[1]:\n",
        "        product_data2 = product\n",
        "\n",
        "    #Now we need to classify if, based on these two product sets, the models are indeed equal\n",
        "   \n",
        "    title1 = standardize_product_title(product_data1['title'])\n",
        "    title2 = standardize_product_title(product_data2['title'])\n",
        "    \n",
        "    if(equal_tv_brands(title1, title2)):\n",
        "      #Are the resolutions equal:\n",
        "      if(equal_resolutions(title1, title2)):\n",
        "        #Are the refresh rates equal\n",
        "        if(equal_refresh_rates(title1, title2)):\n",
        "          #Is the sim at least above the threshold: \n",
        "          idxs1 = np.nonzero(matrix[i1, :])[0].tolist()\n",
        "          idxs2 = np.nonzero(matrix[i2, :])[0].tolist()\n",
        "          sim = calculate_jacard_similarity(set(idxs1), set(idxs2))\n",
        "\n",
        "          if (sim) > sim_threshold:\n",
        "            canidates2.append(pair)\n",
        "\n",
        "#This results in almost 10% of the original canidate pairs, but still have a lot of false positives in here"
      ],
      "metadata": {
        "id": "RU9ba9RCKD_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shingle titles and calculate sim / see what that does\n",
        "dups = []\n",
        "sims = []\n",
        "for pair in canidates3: \n",
        "  i1 = int(pair[0])\n",
        "  i2 = int(pair[1])\n",
        "\n",
        "  #Grab the product key and correspoding store\n",
        "  p1 = str.split(mapping_dict.get(i1),'|||')\n",
        "  p2 = str.split(mapping_dict.get(i2),'|||')\n",
        "\n",
        "  #Get original data set for product 1 \n",
        "  products1 = data.get(p1[0], )\n",
        "  for product in products1: \n",
        "    if product['shop'] == p1[1]:\n",
        "      product_data1 = product\n",
        "\n",
        "  #Get original data set for product 12\n",
        "  products2 = data.get(p2[0], )\n",
        "  for product in products2: \n",
        "    if product['shop'] == p2[1]:\n",
        "      product_data2 = product\n",
        "\n",
        "  #Now we need to classify if, based on these two product sets, the models are indeed equal\n",
        "   \n",
        "  title1 = standardize_product_title(product_data1['title'])\n",
        "  title2 = standardize_product_title(product_data2['title'])\n",
        "\n",
        "  shingles1 = create_shingles(title1, 5)\n",
        "  shingles2 = create_shingles(title2, 5)\n",
        "\n",
        "  sim = calculate_jacard_similarity(shingles1 , shingles2)\n",
        "  if sim > 0.2: \n",
        "    dups.append(pair)\n",
        "  sims.append(sim)\n",
        "\n",
        "sims_array = np.array(sims)\n"
      ],
      "metadata": {
        "id": "82H6W-IaOdCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_ = plt.hist(sims_array, bins='auto')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WJnMz76pPX4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate similarities\n",
        "canidates3 = []\n",
        "sims = []\n",
        "for pair in canidates2: \n",
        "    i1 = int(pair[0])\n",
        "    i2 = int(pair[1])\n",
        "    idxs1 = np.nonzero(matrix[i1, :])[0].tolist()\n",
        "    idxs2 = np.nonzero(matrix[i2, :])[0].tolist()\n",
        "    sim = calculate_jacard_similarity(set(idxs1), set(idxs2))\n",
        "    sims.append(sim)\n",
        "    if (sim) > 0.3:\n",
        "      canidates3.append(pair)"
      ],
      "metadata": {
        "id": "cgvPhrjKMfJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import pdist, jaccard\n",
        "\n",
        "#construct matrix of vectors to be looked at \n",
        "products_added = []\n",
        "df = pd.DataFrame()\n",
        "for pair in canidates2:     \n",
        "    products = [int(pair[0]),int(pair[1]) ]\n",
        "    for product in products:\n",
        "      if product not in products_added:\n",
        "        df[product] = matrix[product, :]\n",
        "        products_added.append(product)\n",
        "\n",
        "df = df.reindex(sorted(df.columns), axis=1)\n",
        "print(df)\n",
        "\n",
        "\n",
        "\n",
        "#distance.jaccard([1,0,0], [1,1,1])"
      ],
      "metadata": {
        "id": "6_szY7uXRhsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "dis_matrix = pdist(df, 'jaccard')\n",
        "# dis_matrix = squareform(dis_matrix)\n",
        "# dis_matrix = pd.DataFrame(dis_matrix, index=df.index, columns = df.index)\n",
        "\n",
        "# Write the DataFrame to CSV file.\n",
        "# with open('/content/gdrive/My Drive/results.csv', 'w') as f:\n",
        "#   dis_matrix.to_csv(f)\n",
        "# # dis_matrix.to_csv('data.csv')\n",
        "# # !cp data.csv \"/content/gdrive/My Drive/\"\n",
        "# print(dis_matrix)"
      ],
      "metadata": {
        "id": "nhcZU_MSWdkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster import hierarchy\n",
        "from scipy.spatial import distance\n",
        "\n",
        "dis_matrix = pdist(df, 'jaccard')\n",
        "dissimilarity = dis_matrix\n",
        "threshold = 10000\n",
        "linkage = hierarchy.single(dissimilarity)\n",
        "clusters = hierarchy.fcluster(linkage, threshold)\n"
      ],
      "metadata": {
        "id": "KZ_X-Pt2ZwE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(clusters)\n",
        "print(clusters.shape)"
      ],
      "metadata": {
        "id": "CXyySctZbQLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluate_pairs(canidates))\n",
        "print(evaluate_pairs(canidates2))\n",
        "print(evaluate_pairs(canidates3))\n",
        "print(evaluate_pairs(dups))"
      ],
      "metadata": {
        "id": "NfCbw5NnZds_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(matrix[:, 1]))"
      ],
      "metadata": {
        "id": "x_f3ePQ2emWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = data.get('PN60F5300AFXZA')\n",
        "store = 'newegg.com'\n",
        "for model in models: \n",
        "  if model['shop'] == store: \n",
        "    pprint(model)"
      ],
      "metadata": {
        "id": "uyfwZm-PQa_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhZ8bM_WDKtC"
      },
      "source": [
        "#Calculate true amount of duplicates in data: \n",
        "\n",
        "duplicate_list = []\n",
        "duplicates =0 \n",
        "counter = 0\n",
        "for pair in canidates: \n",
        "# for i in range(1):\n",
        "#   pair = canidate_pairs[2282]\n",
        "  i1 = int(pair[0])\n",
        "  i2 = int(pair[1])\n",
        "  if(i1 == i2):\n",
        "    pass\n",
        "  else:\n",
        "\n",
        "\n",
        "    p1 = str.split(mapping_dict.get(i1),'|||')\n",
        "    p2 = str.split(mapping_dict.get(i2),'|||')\n",
        "\n",
        "    data1 = data.get(p1[0])\n",
        "    data2 = data.get(p2[0])\n",
        "\n",
        "    model1 = data.get(p1[0])[0]['modelID']\n",
        "    model2 = data.get(p2[0])[0]['modelID']\n",
        "\n",
        "    if (model1 == model2):\n",
        "      duplicates+=1\n",
        "      duplicate_list.append(str([i1, i2]))\n",
        "\n",
        "      idxs1 = np.nonzero(matrix[i1, :])[0].tolist()\n",
        "      idxs2 = np.nonzero(matrix[i2, :])[0].tolist()\n",
        "\n",
        "      sim = calculate_jacard_similarity(set(idxs1), set(idxs2))\n",
        "  counter+=1\n",
        "\n",
        "print(len(duplicate_list))\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(sims)/len(sims))"
      ],
      "metadata": {
        "id": "NsbYWTxIMYWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(vocab_dict.keys())[list(vocab_dict.values()).index(371)])"
      ],
      "metadata": {
        "id": "DgFZ78FqLYVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P45O_txNuwVu"
      },
      "source": [
        "pprint(data.get('TC-P55GT50'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKCCImXytY0Y"
      },
      "source": [
        "#Get similiraty between pairs: \n",
        "duplicates = 0 \n",
        "counter = 0\n",
        "clean_pairs = []\n",
        "\n",
        " \n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvTCfR-xt59k"
      },
      "source": [
        "pprint(data.get('TH55LRU50'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shvTG7Vqi1v-"
      },
      "source": [
        "res = matrix[1,:]\n",
        "idxs = np.nonzero(res)[0].tolist()\n",
        "\n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4OEkjyhdZI"
      },
      "source": [
        "<h1>TESTING CODE<h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuXHxuNmiZVq"
      },
      "source": [
        "# Test similarities\n",
        "set1 = shingle_dict[\"LC-90LE657U|||bestbuy.com\"]\n",
        "set2 = shingle_dict[\"PN60F5300AFXZA|||bestbuy.com\"]\n",
        "set3 = shingle_dict[\"PN60F5300AFXZA|||newegg.com\"]\n",
        "\n",
        "sig1 = signature_matrix[\"LC-90LE657U|||bestbuy.com\"]\n",
        "sig2 = signature_matrix[\"PN60F5300AFXZA|||bestbuy.com\"]\n",
        "sig3 = signature_matrix[\"PN60F5300AFXZA|||newegg.com\"]\n",
        "\n",
        "# Similarity for shingles, we expect the first one to be lower, second one to be higher\n",
        "print(\"Shingle Similarity\")\n",
        "print(calculate_jacard_similarity(set1, set2))\n",
        "print(calculate_jacard_similarity(set1, set3))\n",
        "print(calculate_jacard_similarity(set2, set3))\n",
        "\n",
        "# Similarity for signatures, we expect the first one to be lower, second one to be higher\n",
        "print(\"Signature Similarity\")\n",
        "print(calculate_jacard_similarity(set(sig1), set(sig2)))\n",
        "print(calculate_jacard_similarity(set(sig1), set(sig3)))\n",
        "print(calculate_jacard_similarity(set(sig2), set(sig3)))\n",
        "\n",
        "# Similarity detected using LSH\n",
        "print(\"LSH Similarity\")\n",
        "print(\"----1 AND 2----\")\n",
        "detect_equal_signature(list(sig1), list(sig2))\n",
        "print(\"----1 AND 3----\")\n",
        "detect_equal_signature(list(sig1), list(sig3))\n",
        "print(\"----2 AND 3----\")\n",
        "detect_equal_signature(list(sig2), list(sig3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdKbE6Ed4HVY"
      },
      "source": [
        "###TESTING\n",
        "\n",
        "test = \"TH-65LRU60|||bestbuy.com\"\n",
        "print(test.split(\"|||\")[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zye8TJe4KGu"
      },
      "source": [
        "# Generate dictionary of buckets \n",
        "buckets = {}\n",
        "for product in signature_matrix:\n",
        "  product_store =  product.split(\"|||\")[1]\n",
        "  signature = signature_matrix[product]\n",
        "  bands = create_bands(signature, b)\n",
        "  #print(f\"{product}\")\n",
        "  for band in bands:\n",
        "    if band in buckets:\n",
        "      #print(f\"In bucket{buckets[band]}\")\n",
        "      #Generate the existing stores in this bucket\n",
        "      stores = []\n",
        "      for product_in_band in buckets[band]:\n",
        "          store = product_in_band.split(\"|||\")[1]\n",
        "          stores.append(store)\n",
        "          #print(f\"{product_store} > {store} \")\n",
        "      #if the \"to be added\" product is not equal to already existing stores, we add it\n",
        "      if product_store not in stores:\n",
        "          buckets[band].append(product)\n",
        "    else: \n",
        "      buckets.update({band : [product]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJiVapcKY4-9"
      },
      "source": [
        "def generate_canidate_pairs(canidates):\n",
        "  canidate_pairs = []\n",
        "  for canidate_bucket in canidates:\n",
        "    canidate_list = canidate_bucket\n",
        "    n = len(canidate_list)\n",
        "    for i in range(n):\n",
        "      for j in range(i+1, n):\n",
        "        canidate_pairs.append([canidate_list[i],canidate_list[j]])\n",
        "\n",
        "  return canidate_pairs\n",
        "\n",
        "\n",
        "\n",
        "test = [[1,2,3], [4,5], [6,7,8,9,10]]\n",
        "print(generate_canidate_pairs(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwKGhQGTAur9"
      },
      "source": [
        "title = \"Samsung 60\\\" Class 59910\\\" Diag. Plasma 1080p 600Hz HDTV PN60F5300AFXZA 012354 @@((@* - Best Buy\"\n",
        "\n",
        "print(title)\n",
        "#Standardize inch values\n",
        "inch_values = [\"Inch\", 'inches', '\\\"', '-inch', ' inch']\n",
        "for inch_value in inch_values:\n",
        "  title = title.replace(inch_value, 'inch')\n",
        "\n",
        "#Standardize Hz values\n",
        "hz_values = [\"Hertz\", 'hertz', 'Hz', 'HZ', '-hz']\n",
        "for hz_value in hz_values:\n",
        "  title = title.replace(hz_value, 'hz')\n",
        "\n",
        "print(title)\n",
        "\n",
        "model_words = re.findall(\"[a-zA-Z0-9]+\", title)\n",
        "print(model_words)\n",
        "\n",
        "\n",
        "words_1 = [string for string in model_words if not string.isalpha()]\n",
        "words = [string for string in words_1 if not string.isnumeric()]\n",
        "print(words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWpZGG7ZN0NW"
      },
      "source": [
        "from sympy import isprime\n",
        "import random\n",
        "\n",
        "# We have to save our hash functions and reuse them, so we have 20 different hash functions. Not 20x how ever many sparse vectors we have\n",
        "def create_hash_funcs(vocab_size, m):\n",
        "    hashes = np.zeros((m, vocab_size))\n",
        "    primes = [i for i in range(m,m*m) if isprime(i)]\n",
        "    for i in range(m):\n",
        "        a = random.randint(1, 100)\n",
        "        b = random.randint(1, 100)\n",
        "        p = np.repeat(random.choice(primes), vocab_size)\n",
        "        f = lambda x: a+b*x\n",
        "        base = np.arange(vocab_size)+1\n",
        "        func = f(base)\n",
        "        hash = np.mod(func, p)\n",
        "        hashes[i, :] = hash.copy()    \n",
        "    return hashes.astype(int)\n",
        "\n",
        "\n",
        "result = create_hash_funcs(100, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHLVcCydSBeC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwiyr4iZSCWN"
      },
      "source": [
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "#_---------------------------------------------------------BACK UP.    -------------------------------------------#\n",
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "# Hyperparams\n",
        "#k = 5  # shingle length\n",
        "m = 256  # minhash dense vector length\n",
        "b = 32  # amount of bands for LSH (make sure m is a multiple of b)\n",
        "\n",
        "# We start with a simple version: just save the title and model and shingel > minhash > lsh based on title\n",
        "\n",
        "# This part extracts all the titles and adds them to a dict\n",
        "# ISSUE: duplicate items get filed under the same key, but we want different keys here for comparisons >  SOLUTION use key + shop as unique identifier\n",
        "\n",
        "title_dict = create_data_dict(data)\n",
        "print(\"------ EXTRACTED ALL THE TITLES -------\")\n",
        "\n",
        "# Now we create a list of all our model words and also create our vocab for one hot encoding\n",
        "model_words_dict = {}\n",
        "vocab = set()\n",
        "for key in title_dict:\n",
        "    title = title_dict[key]\n",
        "    model_words = create_model_words(title)\n",
        "    model_words_dict.update({key: model_words})\n",
        "    vocab = set.union(vocab, model_words)\n",
        "print(\"------ CREATED SHINGLES -------\")\n",
        "# print(\"VOCAB: \", vocab)\n",
        "pprint(model_words_dict)\n",
        "\n",
        "\n",
        "total_model_words = len(vocab)\n",
        "\n",
        "# CREATE THE MINHASH MATRIX\n",
        "# we need a dictionary for the vocab to get O(1) access time\n",
        "vocab_dict = {}\n",
        "\n",
        "i = 0\n",
        "for model_word in vocab:\n",
        "    vocab_dict.update({model_word: i})\n",
        "    i = i+1\n",
        "\n",
        "matrix = pd.DataFrame()\n",
        "for key in model_words_dict:\n",
        "    model_words_set = model_words_dict[key]\n",
        "    matrix[key] = encode_one_hot(model_words_set, vocab_dict)\n",
        "print(\"------ CREATED MATRIX SPARSE VEC REPRESENTATION -------\")\n",
        "print(matrix)\n",
        "\n",
        "\n",
        "# Create MinHash Matrix with signature of length m\n",
        "signature_matrix = pd.DataFrame()\n",
        "hashes = create_hash_funcs(total_model_words, m)\n",
        "for column in matrix:\n",
        "    signature_matrix[column] = (create_minhash_signature_vector(\n",
        "        matrix[column], total_model_words, m, hashes))\n",
        "\n",
        "print(\"------ CREATED MINHASH DATAFRAME -------\")\n",
        "\n",
        "print(signature_matrix)\n",
        "\n",
        "\n",
        "# Generate dictionary of buckets \n",
        "buckets = {}\n",
        "for product in signature_matrix:\n",
        "  signature = signature_matrix[product]\n",
        "  bands = create_bands(signature, b)\n",
        "  #print(f\"{product}\")\n",
        "  for band in bands:\n",
        "    if band in buckets:\n",
        "      buckets[band].append(product)\n",
        "    else: \n",
        "      buckets.update({band : [product]})\n",
        "\n",
        "\n",
        "print(\"------ CREATED LSH BUCKETS -------\")\n",
        "#pprint(buckets)\n",
        "\n",
        "#get canidate pairs\n",
        "canidates = []\n",
        "for band in buckets:\n",
        "  if len(buckets[band]) > 1:\n",
        "    for product in buckets[band]:\n",
        "      break\n",
        "    canidates.append(str(buckets[band]))\n",
        "\n",
        "unique_canidates = set(canidates)\n",
        "\n",
        "print(\"------ CREATED CANIDATE PAIRS -------\")\n",
        "print(unique_canidates)\n",
        "print(len(unique_canidates))\n",
        "\n",
        "  \n",
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "#_---------------------------------------------------------FUNCTIONS----------------------------------------------#\n",
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "# Extract data\n",
        "\n",
        "def create_data_dict(data):\n",
        "  dict = {}\n",
        "  for key in data:\n",
        "      products = data[key]\n",
        "      for i in range(0, len(products)):\n",
        "          title = data[key][i].get('title')\n",
        "          dict.update({key + \"|||\" + data[key][i].get('shop'): title})\n",
        "  return dict\n",
        "\n",
        "def create_model_words(title):\n",
        "\n",
        "  #Standardize inch values\n",
        "  inch_values = [\"Inch\", 'inches', '\\\"', '-inch', ' inch']\n",
        "  for inch_value in inch_values:\n",
        "    title = title.replace(inch_value, 'inch')\n",
        "\n",
        "  #Standardize Hz values\n",
        "  hz_values = [\"Hertz\", 'hertz', 'Hz', 'HZ', '-hz']\n",
        "  for hz_value in hz_values:\n",
        "    title = title.replace(hz_value, 'hz')\n",
        "\n",
        "  #Remove site titles\n",
        "  sites = ['TheNerds.net', 'Newegg.com']\n",
        "  for site in sites:\n",
        "    title = title.replace(site, '')\n",
        "\n",
        "  #All to lower case\n",
        "  title = title.lower()\n",
        "  \n",
        "\n",
        "  model_words = re.findall(\"[a-zA-Z0-9\\.]+\", title)\n",
        "  words_1 = [string for string in model_words if not string.isalpha()]\n",
        "  words = [string for string in words_1 if not string.isnumeric()]\n",
        "\n",
        "  return words\n",
        "\n",
        "# Shingling function\n",
        "def create_shingles(doc, k):\n",
        "    shingles = set()\n",
        "    length = len(doc)\n",
        "    for index in range(length-k+1):\n",
        "        shingle = doc[index:index+k]\n",
        "        shingles.add(shingle)\n",
        "\n",
        "    return shingles\n",
        "\n",
        "\n",
        "# one hot encode shingle\n",
        "def encode_one_hot(shingles, vocab):\n",
        "    sparse_vec = [0]*len(vocab)\n",
        "    for shingle in shingles:\n",
        "        idx = vocab.get(shingle)\n",
        "        sparse_vec[idx] = 1\n",
        "        # vocab_size = size of the vocab\n",
        "    return sparse_vec\n",
        "\n",
        "\n",
        "def create_hash_funcs(vocab_size, m):\n",
        "    hashes = []\n",
        "    for _ in range(m):\n",
        "        hash = list(range(1, vocab_size+1))\n",
        "        shuffle(hash)\n",
        "        hash_dict = {}\n",
        "        i = 0\n",
        "        for val in hash:\n",
        "            hash_dict.update({val: i})\n",
        "            i = i+1\n",
        "        \n",
        "        hashes.append(hash_dict)\n",
        "\n",
        "        \n",
        "    return hashes\n",
        "\n",
        "# create signatures\n",
        "# hash = a hash from create_hash_func\n",
        "# sparse_vec = the original sparse vector\n",
        "# vocab_size = size of the vocabulary\n",
        "\n",
        "# We have to save our hash functions and reuse them, so we have 20 different hash functions. Not 20x how ever many sparse vectors we have\n",
        "\n",
        "\n",
        "def create_minhash_signature(sparse_vec, vocab_size, hash):\n",
        "    for i in range(1, vocab_size+1):\n",
        "        idx = hash.get(i)\n",
        "        val = sparse_vec[idx]\n",
        "        # print(f\"{i} > {idx} > {val}\")\n",
        "        if val == 1:\n",
        "            return idx\n",
        "\n",
        "# create complete signature vector\n",
        "\n",
        "\n",
        "def create_minhash_signature_vector(sparse_vec, vocab_size, m, hashes):\n",
        "    signatures = []\n",
        "    for i in range(m):\n",
        "        signature = create_minhash_signature(sparse_vec, vocab_size, hashes[i])\n",
        "        signatures.append(signature)\n",
        "    return signatures\n",
        "\n",
        "# calculate Jacard Similarity\n",
        "\n",
        "\n",
        "def calculate_jacard_similarity(a, b):\n",
        "    return len(a.intersection(b)) / len(a.union(b))\n",
        "\n",
        "\n",
        "# split vector into bands\n",
        "def create_bands(signature, b):\n",
        "    signature = list(signature)\n",
        "    r = int(len(signature)/b)\n",
        "    bands = []\n",
        "    for i in range(0, len(signature), r):\n",
        "        val = str(signature[i: i+r])\n",
        "        bands.append(val)\n",
        "    return bands\n",
        "\n",
        "\n",
        "\n",
        "def generate_canidate_pairs(canidates):\n",
        "  canidate_pairs = []\n",
        "  for canidate_bucket in canidates:\n",
        "    canidate_list = canidate_bucket.strip('][').split(', ')\n",
        "    n = len(canidate_list)\n",
        "    for i in range(n):\n",
        "      for j in range(i+1, n):\n",
        "        canidate_pairs.append([canidate_list[i],canidate_list[j]])\n",
        "\n",
        "  return canidate_pairs\n",
        "\n",
        "# find match between bands, only need 1 band to match to hash into the same bucket\n",
        "\n",
        "\n",
        "def detect_equal_signature(sig1, sig2):\n",
        "    bands1 = split_signature(sig1, b)\n",
        "    bands2 = split_signature(sig2, b)\n",
        "    for band_1, band_2 in zip(bands1, bands2):\n",
        "        if band_1 == band_2:\n",
        "            print(\"MATCH FOUND\")\n",
        "            break\n",
        "\n",
        "# Generate dictionary of buckets \n",
        "buckets = {}\n",
        "i = 0\n",
        "for signature in signature_matrix:\n",
        "  store_to_place =  str.split(mapping_dict.get(i),'|||')[1]\n",
        "  bands = create_bands(signature, b)\n",
        "  for band in bands:\n",
        "    if band in buckets:\n",
        "      #print(\"NEW BAND\")\n",
        "      for product in buckets[band]:\n",
        "        store = str.split(mapping_dict.get(product),'|||')[1]\n",
        "        #print(f\"{store_to_place} > {store}\")\n",
        "        if(store_to_place == store):\n",
        "          buckets.update({band : [i]})\n",
        "          break\n",
        "      buckets[band].append(i)\n",
        "    else: \n",
        "      buckets.update({band : [i]})\n",
        "  i+=1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}